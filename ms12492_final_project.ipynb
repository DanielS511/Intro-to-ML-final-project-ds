{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML Final Project\n",
    "Daniel Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle as pkl\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyLoadDataset(Dataset):\n",
    "  def __init__(self, path, train=True, transform=None):\n",
    "    self.transform = transform\n",
    "    path = path + (\"train/\" if train else \"test/\")\n",
    "\n",
    "    self.pathX = path + \"X/\"\n",
    "    self.pathY = path + \"Y/\"\n",
    "\n",
    "    self.data = os.listdir(self.pathX)\n",
    "    self.train = train\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    f = self.data[idx]\n",
    "\n",
    "    img0 = cv2.imread(self.pathX + f + \"/rgb/0.png\")\n",
    "    img1 = cv2.imread(self.pathX + f + \"/rgb/1.png\")\n",
    "    img2 = cv2.imread(self.pathX + f + \"/rgb/2.png\")\n",
    "\n",
    "    if self.transform is not None:\n",
    "      img0 = self.transform(img0)\n",
    "      img1 = self.transform(img1)\n",
    "      img2 = self.transform(img2)\n",
    "    \n",
    "    depth = np.load(self.pathX + f + \"/depth.npy\") \n",
    "    # depth = self.transform(depth)\n",
    "\n",
    "    field_id = pkl.load(open(self.pathX + f + \"/field_id.pkl\", \"rb\"))\n",
    "\n",
    "    \n",
    "    Y = np.load(self.pathY + f + \".npy\") * 1000\n",
    "    return (img0, img1, img2, depth), Y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.Resize((224, 224)),#attention \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                            [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LazyLoadDataset(\"./lazydata/\", transform=data_transforms['train'])\n",
    "test_dataset = LazyLoadDataset(\"./lazydata/\", transform=data_transforms['test'], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 12, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "  print(batch_idx)\n",
    "  # print(data.shape)\n",
    "  # print(target.shape)\n",
    "  # img0, img1, img2, depth, field_id = data\n",
    "  img0, img1, img2, depth = data\n",
    "  print(img0.shape)\n",
    "  print(img1.shape)\n",
    "  print(img1.shape)\n",
    "  \n",
    "  print(depth.shape)\n",
    "  concate = torch.cat((img0, img1, img2, depth), dim=1)\n",
    "  print(concate.shape)\n",
    "  # print(depth.shape)\n",
    "  # print(len(field_id))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, permute_pixels=None, permutation_order=None):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        epoch (int): current epoch\n",
    "        model (nn.Module): model to train\n",
    "        optimizer (torch.optim): optimizer to use\n",
    "        permute_pixels (function): function to permute the pixels (default: None)\n",
    "        permutation_order (1D torch array): order of the permutation (default: None)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, ((img0, img1, img2, depth), target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        concate = torch.cat((img0, img1, img2, depth), dim=1)\n",
    "        data, target = concate.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        if permute_pixels is not None:\n",
    "            data = permute_pixels(data, permutation_order)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model = model.to(device)\n",
    "        output = model(data)\n",
    "        lossFn = nn.MSELoss()\n",
    "        loss = lossFn(output.float(), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    # return number of parameters in model\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.float()\n",
    "model.fc = nn.Linear(2048, 12)\n",
    "weight = model.conv1.weight.clone()\n",
    "model.conv1 = nn.Conv2d(12, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "with torch.no_grad():\n",
    "    model.conv1.weight[:, :3] = weight\n",
    "    model.conv1.weight[:, 3] = model.conv1.weight[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 23560844\n",
      "Train Epoch: 0 [0/3396 (0%)]\tLoss: 4896.290039\n",
      "Train Epoch: 0 [200/3396 (6%)]\tLoss: 986.163757\n",
      "Train Epoch: 0 [400/3396 (12%)]\tLoss: 175.321091\n",
      "Train Epoch: 0 [600/3396 (18%)]\tLoss: 150.519119\n",
      "Train Epoch: 0 [800/3396 (24%)]\tLoss: 166.161102\n",
      "Train Epoch: 0 [1000/3396 (29%)]\tLoss: 214.571442\n",
      "Train Epoch: 0 [1200/3396 (35%)]\tLoss: 202.220627\n",
      "Train Epoch: 0 [1400/3396 (41%)]\tLoss: 168.664307\n",
      "Train Epoch: 0 [1600/3396 (47%)]\tLoss: 68.875290\n",
      "Train Epoch: 0 [1800/3396 (53%)]\tLoss: 208.548874\n",
      "Train Epoch: 0 [2000/3396 (59%)]\tLoss: 71.504105\n",
      "Train Epoch: 0 [2200/3396 (65%)]\tLoss: 113.864647\n",
      "Train Epoch: 0 [2400/3396 (71%)]\tLoss: 122.327354\n",
      "Train Epoch: 0 [2600/3396 (77%)]\tLoss: 74.356499\n",
      "Train Epoch: 0 [2800/3396 (82%)]\tLoss: 67.508896\n",
      "Train Epoch: 0 [3000/3396 (88%)]\tLoss: 166.657532\n",
      "Train Epoch: 0 [3200/3396 (94%)]\tLoss: 53.277542\n",
      "Train Epoch: 1 [0/3396 (0%)]\tLoss: 85.017830\n",
      "Train Epoch: 1 [200/3396 (6%)]\tLoss: 32.903793\n",
      "Train Epoch: 1 [400/3396 (12%)]\tLoss: 28.603561\n",
      "Train Epoch: 1 [600/3396 (18%)]\tLoss: 54.636086\n",
      "Train Epoch: 1 [800/3396 (24%)]\tLoss: 32.312126\n",
      "Train Epoch: 1 [1000/3396 (29%)]\tLoss: 66.500633\n",
      "Train Epoch: 1 [1200/3396 (35%)]\tLoss: 42.258423\n",
      "Train Epoch: 1 [1400/3396 (41%)]\tLoss: 47.821686\n",
      "Train Epoch: 1 [1600/3396 (47%)]\tLoss: 16.936646\n",
      "Train Epoch: 1 [1800/3396 (53%)]\tLoss: 102.144493\n",
      "Train Epoch: 1 [2000/3396 (59%)]\tLoss: 17.835180\n",
      "Train Epoch: 1 [2200/3396 (65%)]\tLoss: 126.879257\n",
      "Train Epoch: 1 [2400/3396 (71%)]\tLoss: 82.255386\n",
      "Train Epoch: 1 [2600/3396 (77%)]\tLoss: 53.712082\n",
      "Train Epoch: 1 [2800/3396 (82%)]\tLoss: 60.394066\n",
      "Train Epoch: 1 [3000/3396 (88%)]\tLoss: 65.227478\n",
      "Train Epoch: 1 [3200/3396 (94%)]\tLoss: 37.256058\n",
      "Train Epoch: 2 [0/3396 (0%)]\tLoss: 46.603680\n",
      "Train Epoch: 2 [200/3396 (6%)]\tLoss: 19.021465\n",
      "Train Epoch: 2 [400/3396 (12%)]\tLoss: 12.921780\n",
      "Train Epoch: 2 [600/3396 (18%)]\tLoss: 28.952068\n",
      "Train Epoch: 2 [800/3396 (24%)]\tLoss: 23.548443\n",
      "Train Epoch: 2 [1000/3396 (29%)]\tLoss: 40.857216\n",
      "Train Epoch: 2 [1200/3396 (35%)]\tLoss: 26.467154\n",
      "Train Epoch: 2 [1400/3396 (41%)]\tLoss: 21.737188\n",
      "Train Epoch: 2 [1600/3396 (47%)]\tLoss: 16.627666\n",
      "Train Epoch: 2 [1800/3396 (53%)]\tLoss: 82.459023\n",
      "Train Epoch: 2 [2000/3396 (59%)]\tLoss: 10.615494\n",
      "Train Epoch: 2 [2200/3396 (65%)]\tLoss: 81.174004\n",
      "Train Epoch: 2 [2400/3396 (71%)]\tLoss: 68.022911\n",
      "Train Epoch: 2 [2600/3396 (77%)]\tLoss: 60.157608\n",
      "Train Epoch: 2 [2800/3396 (82%)]\tLoss: 43.280910\n",
      "Train Epoch: 2 [3000/3396 (88%)]\tLoss: 49.982704\n",
      "Train Epoch: 2 [3200/3396 (94%)]\tLoss: 35.752041\n",
      "Train Epoch: 3 [0/3396 (0%)]\tLoss: 36.445019\n",
      "Train Epoch: 3 [200/3396 (6%)]\tLoss: 23.316925\n",
      "Train Epoch: 3 [400/3396 (12%)]\tLoss: 8.134048\n",
      "Train Epoch: 3 [600/3396 (18%)]\tLoss: 31.009436\n",
      "Train Epoch: 3 [800/3396 (24%)]\tLoss: 19.845634\n",
      "Train Epoch: 3 [1000/3396 (29%)]\tLoss: 31.416103\n",
      "Train Epoch: 3 [1200/3396 (35%)]\tLoss: 18.346359\n",
      "Train Epoch: 3 [1400/3396 (41%)]\tLoss: 15.065730\n",
      "Train Epoch: 3 [1600/3396 (47%)]\tLoss: 18.538794\n",
      "Train Epoch: 3 [1800/3396 (53%)]\tLoss: 73.674858\n",
      "Train Epoch: 3 [2000/3396 (59%)]\tLoss: 10.313700\n",
      "Train Epoch: 3 [2200/3396 (65%)]\tLoss: 65.426926\n",
      "Train Epoch: 3 [2400/3396 (71%)]\tLoss: 54.792732\n",
      "Train Epoch: 3 [2600/3396 (77%)]\tLoss: 58.769012\n",
      "Train Epoch: 3 [2800/3396 (82%)]\tLoss: 38.338242\n",
      "Train Epoch: 3 [3000/3396 (88%)]\tLoss: 39.445564\n",
      "Train Epoch: 3 [3200/3396 (94%)]\tLoss: 31.925596\n",
      "Train Epoch: 4 [0/3396 (0%)]\tLoss: 32.176605\n",
      "Train Epoch: 4 [200/3396 (6%)]\tLoss: 19.210953\n",
      "Train Epoch: 4 [400/3396 (12%)]\tLoss: 5.706364\n",
      "Train Epoch: 4 [600/3396 (18%)]\tLoss: 28.741558\n",
      "Train Epoch: 4 [800/3396 (24%)]\tLoss: 14.686920\n",
      "Train Epoch: 4 [1000/3396 (29%)]\tLoss: 25.408241\n",
      "Train Epoch: 4 [1200/3396 (35%)]\tLoss: 16.391275\n",
      "Train Epoch: 4 [1400/3396 (41%)]\tLoss: 14.003428\n",
      "Train Epoch: 4 [1600/3396 (47%)]\tLoss: 18.897236\n",
      "Train Epoch: 4 [1800/3396 (53%)]\tLoss: 57.616371\n",
      "Train Epoch: 4 [2000/3396 (59%)]\tLoss: 8.806032\n",
      "Train Epoch: 4 [2200/3396 (65%)]\tLoss: 57.118706\n",
      "Train Epoch: 4 [2400/3396 (71%)]\tLoss: 40.086037\n",
      "Train Epoch: 4 [2600/3396 (77%)]\tLoss: 48.857704\n",
      "Train Epoch: 4 [2800/3396 (82%)]\tLoss: 33.735802\n",
      "Train Epoch: 4 [3000/3396 (88%)]\tLoss: 28.382919\n",
      "Train Epoch: 4 [3200/3396 (94%)]\tLoss: 28.171133\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))\n",
    "\n",
    "test_accuracy = []\n",
    "for epoch in range(0, 5):\n",
    "    train(epoch, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to csv file submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "outfile = 'submission.csv'\n",
    "\n",
    "output_file = open(outfile, 'w')\n",
    "\n",
    "titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
    "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
    "preds = []\n",
    "\n",
    "test_data = torch.load('./test/test/testX.pt')\n",
    "file_ids = test_data[-1]\n",
    "rgb_data = test_data[0]\n",
    "depth = test_data[1]\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(rgb_data):\n",
    "    # Please remember to modify this loop, input and output based on your model/architecture\n",
    "    data = data.view(-1, 224, 224)\n",
    "    data = torch.cat((data, depth[i]), dim=0)\n",
    "    data = torch.unsqueeze(data, 0)\n",
    "    output = model(data.to('cuda')) /1000\n",
    "    preds.append(output[0].cpu().detach().numpy())\n",
    "\n",
    "df = pd.concat([pd.DataFrame(file_ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n",
    "df.columns = titles\n",
    "df.to_csv(outfile, index = False)\n",
    "print(\"Written to csv file {}\".format(outfile))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98df1a0825261fa5da2004b0c92c481089275adf1d4b1391f737350e394b16e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
